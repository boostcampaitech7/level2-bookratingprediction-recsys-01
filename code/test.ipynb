{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import regex\n",
    "import glob\n",
    "\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/ephemeral/home/jay/data/'\n",
    "users = pd.read_csv(data_path + 'users.csv')\n",
    "books = pd.read_csv(data_path + 'books.csv')\n",
    "train = pd.read_csv(data_path + 'train_ratings.csv')\n",
    "test = pd.read_csv(data_path + 'test_ratings.csv')\n",
    "sub = pd.read_csv(data_path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def str2list(x: str) -> list:\n",
    "    '''문자열을 리스트로 변환하는 함수'''\n",
    "    return x[1:-1].split(', ')\n",
    "\n",
    "\n",
    "def split_location(x: str) -> list:\n",
    "    '''\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : str\n",
    "        location 데이터\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    res : list\n",
    "        location 데이터를 나눈 뒤, 정제한 결과를 반환합니다.\n",
    "        순서는 country, state, city, ... 입니다.\n",
    "    '''\n",
    "    res = x.split(',')\n",
    "    res = [i.strip().lower() for i in res]\n",
    "    res = [regex.sub(r'[^a-zA-Z/ ]', '', i) for i in res]  # remove special characters\n",
    "    res = [i if i not in ['n/a', ''] else np.nan for i in res]  # change 'n/a' into NaN\n",
    "    res.reverse()  # reverse the list to get country, state, city, ... order\n",
    "\n",
    "    for i in range(len(res)-1, 0, -1):\n",
    "        if (res[i] in res[:i]) and (not pd.isna(res[i])):  # remove duplicated values if not NaN\n",
    "            res.pop(i)\n",
    "\n",
    "    return res\n",
    "    \n",
    "def process_context_data(users, books):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    users : pd.DataFrame\n",
    "        users.csv를 인덱싱한 데이터\n",
    "    books : pd.DataFrame\n",
    "        books.csv를 인덱싱한 데이터\n",
    "    ratings1 : pd.DataFrame\n",
    "        train 데이터의 rating\n",
    "    ratings2 : pd.DataFrame\n",
    "        test 데이터의 rating\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    label_to_idx : dict\n",
    "        데이터를 인덱싱한 정보를 담은 딕셔너리\n",
    "    idx_to_label : dict\n",
    "        인덱스를 다시 원래 데이터로 변환하는 정보를 담은 딕셔너리\n",
    "    train_df : pd.DataFrame\n",
    "        train 데이터\n",
    "    test_df : pd.DataFrame\n",
    "        test 데이터\n",
    "    \"\"\"\n",
    "\n",
    "    users_ = users.copy()\n",
    "    books_ = books.copy()\n",
    "\n",
    "    # 데이터 전처리 (전처리는 각자의 상황에 맞게 진행해주세요!)\n",
    "    books_['category'] = books_['category'].apply(lambda x: str2list(x)[0] if not pd.isna(x) else np.nan)\n",
    "    books_['language'] = books_['language'].fillna(books_['language'].mode()[0])\n",
    "    books_['publication_range'] = books_['year_of_publication'].apply(lambda x: x // 10 * 10)  # 1990년대, 2000년대, 2010년대, ...\n",
    "\n",
    "    users_['age'] = users_['age'].fillna(users_['age'].mode()[0])\n",
    "    users_['age_range'] = users_['age'].apply(lambda x: x // 10 * 10)  # 10대, 20대, 30대, ...\n",
    "\n",
    "    users_['location_list'] = users_['location'].apply(lambda x: split_location(x)) \n",
    "    users_['location_country'] = users_['location_list'].apply(lambda x: x[0])\n",
    "    users_['location_state'] = users_['location_list'].apply(lambda x: x[1] if len(x) > 1 else np.nan)\n",
    "    users_['location_city'] = users_['location_list'].apply(lambda x: x[2] if len(x) > 2 else np.nan)\n",
    "    for idx, row in users_.iterrows():\n",
    "        if (not pd.isna(row['location_state'])) and pd.isna(row['location_country']):\n",
    "            fill_country = users_[users_['location_state'] == row['location_state']]['location_country'].mode()\n",
    "            fill_country = fill_country[0] if len(fill_country) > 0 else np.nan\n",
    "            users_.loc[idx, 'location_country'] = fill_country\n",
    "        elif (not pd.isna(row['location_city'])) and pd.isna(row['location_state']):\n",
    "            if not pd.isna(row['location_country']):\n",
    "                fill_state = users_[(users_['location_country'] == row['location_country']) \n",
    "                                    & (users_['location_city'] == row['location_city'])]['location_state'].mode()\n",
    "                fill_state = fill_state[0] if len(fill_state) > 0 else np.nan\n",
    "                users_.loc[idx, 'location_state'] = fill_state\n",
    "            else:\n",
    "                fill_state = users_[users_['location_city'] == row['location_city']]['location_state'].mode()\n",
    "                fill_state = fill_state[0] if len(fill_state) > 0 else np.nan\n",
    "                fill_country = users_[users_['location_city'] == row['location_city']]['location_country'].mode()\n",
    "                fill_country = fill_country[0] if len(fill_country) > 0 else np.nan\n",
    "                users_.loc[idx, 'location_country'] = fill_country\n",
    "                users_.loc[idx, 'location_state'] = fill_state\n",
    "\n",
    "               \n",
    "    \n",
    "    users_ = users_.drop(['location'], axis=1)\n",
    "\n",
    "    return users_, books_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = ['user_id', 'age_range', 'location_country', 'location_state', 'location_city']\n",
    "book_features = ['isbn', 'book_title', 'book_author', 'publisher', 'language', 'category', 'publication_range']\n",
    "sparse_cols = ['user_id', 'isbn'] + list(set(user_features + book_features) - {'user_id', 'isbn'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_, books_ = process_context_data(users, books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "\n",
    "def image_vector(path, img_size):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        이미지가 존재하는 경로를 입력합니다.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img_fe : np.ndarray\n",
    "        이미지를 벡터화한 결과를 반환합니다.\n",
    "        베이스라인에서는 grayscale일 경우 RGB로 변경한 뒤, img_size x img_size 로 사이즈를 맞추어 numpy로 반환합니다.\n",
    "    \"\"\"\n",
    "    img = Image.open(path)\n",
    "    transform = v2.Compose([\n",
    "        v2.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),\n",
    "        v2.Resize((img_size, img_size)),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return transform(img).numpy()\n",
    "\n",
    "def process_img_data(books):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    books : pd.DataFrame\n",
    "        책 정보에 대한 데이터 프레임을 입력합니다.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    books_ : pd.DataFrame\n",
    "        이미지 정보를 벡터화하여 추가한 데이터 프레임을 반환합니다.\n",
    "    \"\"\"\n",
    "    books_ = books.copy()\n",
    "    books_['img_path'] = books_['img_path'].apply(lambda x: f'../data/{x}')\n",
    "    img_vecs = []\n",
    "    for idx in tqdm(books_.index):\n",
    "        img_vec = image_vector(books_.loc[idx, 'img_path'], 28)\n",
    "        img_vecs.append(img_vec)\n",
    "\n",
    "    books_['img_vector'] = img_vecs\n",
    "\n",
    "    return books_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149570/149570 [01:58<00:00, 1257.39it/s]\n"
     ]
    }
   ],
   "source": [
    "books_ = process_img_data(books_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_features = ['user_id', 'age_range', 'location_country', 'location_state', 'location_city']\n",
    "book_features = ['isbn', 'book_title', 'book_author', 'publisher', 'language', 'category', 'publication_range']\n",
    "sparse_cols = ['user_id', 'isbn'] + list(set(user_features + book_features) - {'user_id', 'isbn'})\n",
    "\n",
    "train_df = train.merge(books_, on='isbn', how='left')\\\n",
    "                .merge(users_, on='user_id', how='left')[sparse_cols + ['img_vector', 'rating']]\n",
    "test_df = test.merge(books_, on='isbn', how='left')\\\n",
    "                .merge(users_, on='user_id', how='left')[sparse_cols + ['img_vector']]\n",
    "all_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# feature_cols의 데이터만 라벨 인코딩하고 인덱스 정보를 저장\n",
    "label2idx, idx2label = {}, {}\n",
    "for col in sparse_cols:\n",
    "    all_df[col] = all_df[col].fillna('unknown')\n",
    "    train_df[col] = train_df[col].fillna('unknown')\n",
    "    test_df[col] = test_df[col].fillna('unknown')\n",
    "    unique_labels = all_df[col].astype(\"category\").cat.categories\n",
    "    label2idx[col] = {label:idx for idx, label in enumerate(unique_labels)}\n",
    "    idx2label[col] = {idx:label for idx, label in enumerate(unique_labels)}\n",
    "    train_df[col] = pd.Categorical(train_df[col], categories=unique_labels).codes\n",
    "    test_df[col] = pd.Categorical(test_df[col], categories=unique_labels).codes\n",
    "\n",
    "field_dims = [len(label2idx[col]) for col in sparse_cols]\n",
    "\n",
    "data = {\n",
    "        'train':train_df,\n",
    "        'test':test_df,\n",
    "        'field_names':sparse_cols,\n",
    "        'field_dims':field_dims,\n",
    "        'label2idx':label2idx,\n",
    "        'idx2label':idx2label,\n",
    "        'sub':sub,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>category</th>\n",
       "      <th>language</th>\n",
       "      <th>location_city</th>\n",
       "      <th>book_title</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publication_range</th>\n",
       "      <th>book_author</th>\n",
       "      <th>location_country</th>\n",
       "      <th>age_range</th>\n",
       "      <th>location_state</th>\n",
       "      <th>img_vector</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>39</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>10695</td>\n",
       "      <td>20344</td>\n",
       "      <td>4598</td>\n",
       "      <td>12</td>\n",
       "      <td>49515</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>987</td>\n",
       "      <td>[[[0.6563062, 0.93030226, 1.4954194, 1.8036649...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>16631</td>\n",
       "      <td>39</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>10752</td>\n",
       "      <td>20344</td>\n",
       "      <td>4598</td>\n",
       "      <td>12</td>\n",
       "      <td>49515</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>987</td>\n",
       "      <td>[[[0.6563062, 0.93030226, 1.4954194, 1.8036649...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>30285</td>\n",
       "      <td>39</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>5368</td>\n",
       "      <td>20344</td>\n",
       "      <td>4598</td>\n",
       "      <td>12</td>\n",
       "      <td>49515</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>987</td>\n",
       "      <td>[[[0.6563062, 0.93030226, 1.4954194, 1.8036649...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>48760</td>\n",
       "      <td>39</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>2284</td>\n",
       "      <td>20344</td>\n",
       "      <td>4598</td>\n",
       "      <td>12</td>\n",
       "      <td>49515</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>987</td>\n",
       "      <td>[[[0.6563062, 0.93030226, 1.4954194, 1.8036649...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>51452</td>\n",
       "      <td>39</td>\n",
       "      <td>132</td>\n",
       "      <td>4</td>\n",
       "      <td>4229</td>\n",
       "      <td>20344</td>\n",
       "      <td>4598</td>\n",
       "      <td>12</td>\n",
       "      <td>49515</td>\n",
       "      <td>34</td>\n",
       "      <td>2</td>\n",
       "      <td>987</td>\n",
       "      <td>[[[0.6563062, 0.93030226, 1.4954194, 1.8036649...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306790</th>\n",
       "      <td>68062</td>\n",
       "      <td>87948</td>\n",
       "      <td>4120</td>\n",
       "      <td>4</td>\n",
       "      <td>8337</td>\n",
       "      <td>111499</td>\n",
       "      <td>9440</td>\n",
       "      <td>12</td>\n",
       "      <td>12391</td>\n",
       "      <td>211</td>\n",
       "      <td>2</td>\n",
       "      <td>222</td>\n",
       "      <td>[[[1.9235382, 1.8721639, 1.9064134, 1.9064134,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306791</th>\n",
       "      <td>68066</td>\n",
       "      <td>75092</td>\n",
       "      <td>2729</td>\n",
       "      <td>4</td>\n",
       "      <td>2564</td>\n",
       "      <td>98380</td>\n",
       "      <td>8086</td>\n",
       "      <td>10</td>\n",
       "      <td>45139</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>1317</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306792</th>\n",
       "      <td>68066</td>\n",
       "      <td>109944</td>\n",
       "      <td>4120</td>\n",
       "      <td>4</td>\n",
       "      <td>2564</td>\n",
       "      <td>131928</td>\n",
       "      <td>6111</td>\n",
       "      <td>10</td>\n",
       "      <td>10197</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>1317</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306793</th>\n",
       "      <td>68066</td>\n",
       "      <td>113224</td>\n",
       "      <td>2298</td>\n",
       "      <td>4</td>\n",
       "      <td>2564</td>\n",
       "      <td>97954</td>\n",
       "      <td>5689</td>\n",
       "      <td>11</td>\n",
       "      <td>27804</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>1317</td>\n",
       "      <td>[[[0.8446785, 0.60493195, 0.72480524, 0.913177...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306794</th>\n",
       "      <td>68066</td>\n",
       "      <td>124459</td>\n",
       "      <td>4120</td>\n",
       "      <td>4</td>\n",
       "      <td>2564</td>\n",
       "      <td>24338</td>\n",
       "      <td>466</td>\n",
       "      <td>11</td>\n",
       "      <td>38272</td>\n",
       "      <td>211</td>\n",
       "      <td>3</td>\n",
       "      <td>1317</td>\n",
       "      <td>[[[-1.278791, -1.2959157, -0.9362959, -0.98767...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306795 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    isbn  category  language  location_city  book_title  \\\n",
       "0             0      39       132         4          10695       20344   \n",
       "1         16631      39       132         4          10752       20344   \n",
       "2         30285      39       132         4           5368       20344   \n",
       "3         48760      39       132         4           2284       20344   \n",
       "4         51452      39       132         4           4229       20344   \n",
       "...         ...     ...       ...       ...            ...         ...   \n",
       "306790    68062   87948      4120         4           8337      111499   \n",
       "306791    68066   75092      2729         4           2564       98380   \n",
       "306792    68066  109944      4120         4           2564      131928   \n",
       "306793    68066  113224      2298         4           2564       97954   \n",
       "306794    68066  124459      4120         4           2564       24338   \n",
       "\n",
       "        publisher  publication_range  book_author  location_country  \\\n",
       "0            4598                 12        49515                34   \n",
       "1            4598                 12        49515                34   \n",
       "2            4598                 12        49515                34   \n",
       "3            4598                 12        49515                34   \n",
       "4            4598                 12        49515                34   \n",
       "...           ...                ...          ...               ...   \n",
       "306790       9440                 12        12391               211   \n",
       "306791       8086                 10        45139               211   \n",
       "306792       6111                 10        10197               211   \n",
       "306793       5689                 11        27804               211   \n",
       "306794        466                 11        38272               211   \n",
       "\n",
       "        age_range  location_state  \\\n",
       "0               2             987   \n",
       "1               3             987   \n",
       "2               2             987   \n",
       "3               2             987   \n",
       "4               2             987   \n",
       "...           ...             ...   \n",
       "306790          2             222   \n",
       "306791          3            1317   \n",
       "306792          3            1317   \n",
       "306793          3            1317   \n",
       "306794          3            1317   \n",
       "\n",
       "                                               img_vector  rating  \n",
       "0       [[[0.6563062, 0.93030226, 1.4954194, 1.8036649...       4  \n",
       "1       [[[0.6563062, 0.93030226, 1.4954194, 1.8036649...       7  \n",
       "2       [[[0.6563062, 0.93030226, 1.4954194, 1.8036649...       8  \n",
       "3       [[[0.6563062, 0.93030226, 1.4954194, 1.8036649...       8  \n",
       "4       [[[0.6563062, 0.93030226, 1.4954194, 1.8036649...       9  \n",
       "...                                                   ...     ...  \n",
       "306790  [[[1.9235382, 1.8721639, 1.9064134, 1.9064134,...       7  \n",
       "306791  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...       6  \n",
       "306792  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...       7  \n",
       "306793  [[[0.8446785, 0.60493195, 0.72480524, 0.913177...       7  \n",
       "306794  [[[-1.278791, -1.2959157, -0.9362959, -0.98767...      10  \n",
       "\n",
       "[306795 rows x 14 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_32 = torch.randn(512, 32)   # torch.Size([512, 32])\n",
    "tensor_64 = torch.randn(512, 64)   # torch.Size([512, 64])\n",
    "tensor_128 = torch.randn(512, 128) # torch.Size([512, 128])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2155,  0.0107,  1.4315,  ..., -0.0577,  0.0609,  0.9538],\n",
       "        [ 0.0318,  0.5450, -0.0040,  ..., -0.0110, -1.3896,  0.4619],\n",
       "        [ 0.1508, -0.7334, -1.7633,  ...,  0.7443,  0.5680, -0.5062],\n",
       "        ...,\n",
       "        [-0.0922,  0.4166,  1.1189,  ...,  1.2173, -0.0697, -0.3387],\n",
       "        [ 0.9076, -0.6142,  0.2010,  ..., -1.3910, -0.9923,  0.8728],\n",
       "        [-0.5993,  1.4854,  1.0638,  ..., -0.4768, -0.8801, -0.9106]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cat([tensor_32, tensor_64, tensor_128], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "import re\n",
    "import os\n",
    "\n",
    "def text_preprocessing(summary):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    summary : pd.Series\n",
    "        정규화와 같은 기본적인 전처리를 하기 위한 텍스트 데이터를 입력합니다.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    summary : pd.Series\n",
    "        전처리된 텍스트 데이터를 반환합니다.\n",
    "        베이스라인에서는 특수문자 제거, 공백 제거를 진행합니다.\n",
    "    \"\"\"\n",
    "    summary = re.sub(\"[^0-9a-zA-Z.,!?]\", \" \", summary)  # .,!?를 제외한 특수문자 제거\n",
    "    summary = re.sub(\"\\s+\", \" \", summary)  # 중복 공백 제거\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "def text_to_vector(text, tokenizer, model):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        `summary_merge()`를 통해 병합된 요약 데이터\n",
    "    tokenizer : Tokenizer\n",
    "        텍스트 데이터를 `model`에 입력하기 위한 토크나이저\n",
    "    model : 사전학습된 언어 모델\n",
    "        텍스트 데이터를 벡터로 임베딩하기 위한 모델\n",
    "    ----------\n",
    "    \"\"\"\n",
    "    text_ = \"[CLS] \" + text + \" [SEP]\"\n",
    "    tokenized = tokenizer.encode(text_, add_special_tokens=True)\n",
    "    token_tensor = torch.tensor([tokenized], device=model.device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(token_tensor)  # attention_mask를 사용하지 않아도 됨\n",
    "        ### BERT 모델의 경우, 최종 출력물의 사이즈가 (토큰길이, 임베딩=768)이므로, 이를 평균내어 사용하거나 pooler_output을 사용하여 [CLS] 토큰의 임베딩만 사용\n",
    "        # sentence_embedding = torch.mean(outputs.last_hidden_state[0], dim=0)  # 방법1) 모든 토큰의 임베딩을 평균내어 사용\n",
    "        sentence_embedding = outputs.pooler_output.squeeze(0)  # 방법2) pooler_output을 사용하여 맨 첫 토큰인 [CLS] 토큰의 임베딩만 사용\n",
    "    \n",
    "    return sentence_embedding.cpu().detach().numpy() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text_data(ratings, users, books, tokenizer, model, vector_create=False):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    users : pd.DataFrame\n",
    "        유저 정보에 대한 데이터 프레임을 입력합니다.\n",
    "    books : pd.DataFrame\n",
    "        책 정보에 대한 데이터 프레임을 입력합니다.\n",
    "    vector_create : bool\n",
    "        사전에 텍스트 데이터 벡터화가 된 파일이 있는지 여부를 입력합니다.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `users_` : pd.DataFrame\n",
    "        각 유저가 읽은 책에 대한 요약 정보를 병합 및 벡터화하여 추가한 데이터 프레임을 반환합니다.\n",
    "\n",
    "    `books_` : pd.DataFrame\n",
    "        텍스트 데이터를 벡터화하여 추가한 데이터 프레임을 반환합니다.\n",
    "    \"\"\"\n",
    "    num2txt = ['Zero', 'One', 'Two', 'Three', 'Four', 'Five']\n",
    "    users_ = users.copy()\n",
    "    books_ = books.copy()\n",
    "    nan_value = 'None'\n",
    "    books_['summary'] = books_['summary'].fillna(nan_value)\\\n",
    "                                         .apply(lambda x: text_preprocessing(x))\\\n",
    "                                         .replace({'': nan_value, ' ': nan_value})\n",
    "    \n",
    "    books_['summary_length'] = books_['summary'].apply(lambda x:len(x))\n",
    "    books_['review_count'] = books_['isbn'].map(ratings['isbn'].value_counts())\n",
    "\n",
    "    users_['books_read'] = users_['user_id'].map(ratings.groupby('user_id')['isbn'].apply(list))\n",
    "\n",
    "    if vector_create:\n",
    "        if not os.path.exists('./data/text_vector'):\n",
    "            os.makedirs('./data/text_vector')\n",
    "\n",
    "        print('Create Item Summary Vector')\n",
    "        book_summary_vector_list = []\n",
    "        for title, summary in tqdm(zip(books_['book_title'], books_['summary']), total=len(books_)):\n",
    "            # 책에 대한 텍스트 프롬프트는 아래와 같이 구성됨\n",
    "            # '''\n",
    "            # Book Title: {title}\n",
    "            # Summary: {summary}\n",
    "            # '''\n",
    "            prompt_ = f'Book Title: {title}\\n Summary: {summary}\\n'\n",
    "            vector = text_to_vector(prompt_, tokenizer, model)\n",
    "            book_summary_vector_list.append(vector)\n",
    "        \n",
    "        book_summary_vector_list = np.concatenate([\n",
    "                                                books_['isbn'].values.reshape(-1, 1),\n",
    "                                                np.asarray(book_summary_vector_list, dtype=np.float32)\n",
    "                                                ], axis=1)\n",
    "        \n",
    "        np.save('./data/text_vector/book_summary_vector.npy', book_summary_vector_list)        \n",
    "\n",
    "\n",
    "        print('Create User Summary Merge Vector')\n",
    "        user_summary_merge_vector_list = []\n",
    "        for books_read in tqdm(users_['books_read']):\n",
    "            if not isinstance(books_read, list) and pd.isna(books_read):  # 유저가 읽은 책이 없는 경우, 텍스트 임베딩을 0으로 처리\n",
    "                user_summary_merge_vector_list.append(np.zeros((768)))\n",
    "                continue\n",
    "            \n",
    "            read_books = books_[books_['isbn'].isin(books_read)][['book_title', 'summary', 'review_count']]\n",
    "            read_books = read_books.sort_values('review_count', ascending=False).head(5)  # review_count가 높은 순으로 5개의 책을 선택\n",
    "            # 유저에 대한 텍스트 프롬프트는 아래와 같이 구성됨\n",
    "            # DeepCoNN에서 유저의 리뷰를 요약하여 하나의 벡터로 만들어 사용함을 참고 (https://arxiv.org/abs/1701.04783)\n",
    "            # '''\n",
    "            # Five Books That You Read\n",
    "            # 1. Book Title: {title}\n",
    "            # Summary: {summary}\n",
    "            # ...\n",
    "            # 5. Book Title: {title}\n",
    "            # Summary: {summary}\n",
    "            # '''\n",
    "            prompt_ = f'{num2txt[len(read_books)]} Books That You Read\\n'\n",
    "            for idx, (title, summary) in enumerate(zip(read_books['book_title'], read_books['summary'])):\n",
    "                summary = summary if len(summary) < 100 else f'{summary[:100]} ...'\n",
    "                prompt_ += f'{idx+1}. Book Title: {title}\\n Summary: {summary}\\n'\n",
    "            vector = text_to_vector(prompt_, tokenizer, model)\n",
    "            user_summary_merge_vector_list.append(vector)\n",
    "        \n",
    "        user_summary_merge_vector_list = np.concatenate([\n",
    "                                                         users_['user_id'].values.reshape(-1, 1),\n",
    "                                                         np.asarray(user_summary_merge_vector_list, dtype=np.float32)\n",
    "                                                        ], axis=1)\n",
    "        \n",
    "        np.save('./data/text_vector/user_summary_merge_vector.npy', user_summary_merge_vector_list)        \n",
    "        \n",
    "    else:\n",
    "        print('Check Vectorizer')\n",
    "        print('Vector Load')\n",
    "        book_summary_vector_list = np.load('/data/ephemeral/home/jay/code/data/text_vector/book_summary_vector.npy', allow_pickle=True)\n",
    "        user_summary_merge_vector_list = np.load('/data/ephemeral/home/jay/code/data/text_vector/user_summary_merge_vector.npy', allow_pickle=True)\n",
    "\n",
    "    book_summary_vector_df = pd.DataFrame({'isbn': book_summary_vector_list[:, 0]})\n",
    "    book_summary_vector_df['book_summary_vector'] = list(book_summary_vector_list[:, 1:].astype(np.float32))\n",
    "    user_summary_vector_df = pd.DataFrame({'user_id': user_summary_merge_vector_list[:, 0]})\n",
    "    user_summary_vector_df['user_summary_merge_vector'] = list(user_summary_merge_vector_list[:, 1:].astype(np.float32))\n",
    "\n",
    "    books_ = pd.merge(books_, book_summary_vector_df, on='isbn', how='left')\n",
    "    users_ = pd.merge(users_, user_summary_vector_df, on='user_id', how='left')\n",
    "\n",
    "    return users_, books_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = '/data/ephemeral/home/jay/data/'\n",
    "users = pd.read_csv(data_path + 'users.csv')\n",
    "books = pd.read_csv(data_path + 'books.csv')\n",
    "train = pd.read_csv(data_path + 'train_ratings.csv')\n",
    "test = pd.read_csv(data_path + 'test_ratings.csv')\n",
    "sub = pd.read_csv(data_path + 'sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_, books_ = process_context_data(users, books)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Vectorizer\n",
      "Vector Load\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'numpy._core'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[52], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModel\u001b[38;5;241m.\u001b[39mfrom_pretrained(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbert-base-uncased\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m users_, books_ \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_text_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musers_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbooks_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 93\u001b[0m, in \u001b[0;36mprocess_text_data\u001b[0;34m(ratings, users, books, tokenizer, model, vector_create)\u001b[0m\n\u001b[1;32m     91\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCheck Vectorizer\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVector Load\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m     book_summary_vector_list \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m/data/ephemeral/home/jay/code/data/text_vector/book_summary_vector.npy\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m     user_summary_merge_vector_list \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/data/ephemeral/home/jay/code/data/text_vector/user_summary_merge_vector.npy\u001b[39m\u001b[38;5;124m'\u001b[39m, allow_pickle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m     96\u001b[0m book_summary_vector_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124misbn\u001b[39m\u001b[38;5;124m'\u001b[39m: book_summary_vector_list[:, \u001b[38;5;241m0\u001b[39m]})\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/npyio.py:456\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding, max_header_size)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mformat\u001b[39m\u001b[38;5;241m.\u001b[39mopen_memmap(file, mode\u001b[38;5;241m=\u001b[39mmmap_mode,\n\u001b[1;32m    454\u001b[0m                                   max_header_size\u001b[38;5;241m=\u001b[39mmax_header_size)\n\u001b[1;32m    455\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mformat\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfid\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mallow_pickle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_pickle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mpickle_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    458\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mmax_header_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmax_header_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;66;03m# Try a pickle\u001b[39;00m\n\u001b[1;32m    461\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m allow_pickle:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/numpy/lib/format.py:800\u001b[0m, in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs, max_header_size)\u001b[0m\n\u001b[1;32m    798\u001b[0m     pickle_kwargs \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    799\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 800\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    801\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m    802\u001b[0m     \u001b[38;5;66;03m# Friendlier error message\u001b[39;00m\n\u001b[1;32m    803\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mUnicodeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnpickling a python object failed: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    804\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou may need to pass the encoding= option \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    805\u001b[0m                        \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto numpy.load\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (err,)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'numpy._core'"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = AutoModel.from_pretrained('bert-base-uncased')\n",
    "model.eval()\n",
    "users_, books_ = process_text_data(train, users_, books_, tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = train.merge(users_, on='user_id', how='left')\\\n",
    "                .merge(books_, on='isbn', how='left')[sparse_cols + ['rating']]\n",
    "test_df = test.merge(users_, on='user_id', how='left')\\\n",
    "                .merge(books_, on='isbn', how='left')[sparse_cols]\n",
    "all_df = pd.concat([train_df, test_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_cols의 데이터만 라벨 인코딩하고 인덱스 정보를 저장\n",
    "label2idx, idx2label = {}, {}\n",
    "for col in sparse_cols:\n",
    "    all_df[col] = all_df[col].fillna('unknown')\n",
    "    unique_labels = all_df[col].astype(\"category\").cat.categories\n",
    "    label2idx[col] = {label:idx for idx, label in enumerate(unique_labels)}\n",
    "    idx2label[col] = {idx:label for idx, label in enumerate(unique_labels)}\n",
    "    train_df[col] = train_df[col].astype(\"category\").cat.codes\n",
    "    test_df[col] = test_df[col].astype(\"category\").cat.codes\n",
    "\n",
    "field_dims = [len(label2idx[col]) for col in train_df.columns if col != 'rating']\n",
    "\n",
    "basic_data = {\n",
    "        'train':train_df,\n",
    "        'test':test_df,\n",
    "        'field_names':sparse_cols,\n",
    "        'field_dims':field_dims,\n",
    "        'label2idx':label2idx,\n",
    "        'idx2label':idx2label,\n",
    "        'sub':sub,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                                                        basic_data['train'].drop(['rating'], axis=1),\n",
    "                                                        basic_data['train']['rating'],\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42,\n",
    "                                                        shuffle=True\n",
    "                                                        )\n",
    "basic_data['X_train'], basic_data['X_valid'], basic_data['y_train'], basic_data['y_valid'] = X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(torch.LongTensor(basic_data['X_train'].values), torch.LongTensor(basic_data['y_train'].values))\n",
    "valid_dataset = TensorDataset(torch.LongTensor(basic_data['X_valid'].values), torch.LongTensor(basic_data['y_valid'].values))\n",
    "test_dataset = TensorDataset(torch.LongTensor(basic_data['test'].values))\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "\n",
    "basic_data['train_dataloader'], basic_data['valid_dataloader'], basic_data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torchvision.transforms import v2\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "#from .basic_data import basic_data_split\n",
    "\n",
    "\n",
    "class Image_Dataset(Dataset):\n",
    "    def __init__(self, user_book_vector, img_vector, rating=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_book_vector : np.ndarray\n",
    "            모델 학습에 사용할 유저 및 책 정보(범주형 데이터)를 입력합니다.\n",
    "        img_vector : np.ndarray\n",
    "            벡터화된 이미지 데이터를 입력합니다.\n",
    "        rating : np.ndarray\n",
    "            정답 데이터를 입력합니다.\n",
    "        \"\"\"\n",
    "        self.user_book_vector = user_book_vector\n",
    "        self.img_vector = img_vector\n",
    "        self.rating = rating\n",
    "    def __len__(self):\n",
    "        return self.user_book_vector.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "                'user_book_vector' : torch.tensor(self.user_book_vector[i], dtype=torch.long),\n",
    "                'img_vector' : torch.tensor(self.img_vector[i], dtype=torch.float32),\n",
    "                'rating' : torch.tensor(self.rating[i], dtype=torch.float32)\n",
    "                } if self.rating is not None else \\\n",
    "                {\n",
    "                'user_book_vector' : torch.tensor(self.user_book_vector[i], dtype=torch.long),\n",
    "                'img_vector' : torch.tensor(self.img_vector[i], dtype=torch.float32)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_vector(path, img_size):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        이미지가 존재하는 경로를 입력합니다.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    img_fe : np.ndarray\n",
    "        이미지를 벡터화한 결과를 반환합니다.\n",
    "        베이스라인에서는 grayscale일 경우 RGB로 변경한 뒤, img_size x img_size 로 사이즈를 맞추어 numpy로 반환합니다.\n",
    "    \"\"\"\n",
    "    img = Image.open(path)\n",
    "    transform = v2.Compose([\n",
    "        v2.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),\n",
    "        v2.Resize((img_size, img_size)),\n",
    "        v2.ToImage(),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "        v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    return transform(img).numpy()\n",
    "\n",
    "\n",
    "def process_img_data(books, args):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    books : pd.DataFrame\n",
    "        책 정보에 대한 데이터 프레임을 입력합니다.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    books_ : pd.DataFrame\n",
    "        이미지 정보를 벡터화하여 추가한 데이터 프레임을 반환합니다.\n",
    "    \"\"\"\n",
    "    books_ = books.copy()\n",
    "    books_['img_path'] = books_['img_path'].apply(lambda x: f'data/{x}')\n",
    "    img_vecs = []\n",
    "    for idx in tqdm(books_.index):\n",
    "        img_vec = image_vector(books_.loc[idx, 'img_path'], args.model_args[args.model].img_size)\n",
    "        img_vecs.append(img_vec)\n",
    "\n",
    "    books_['img_vector'] = img_vecs\n",
    "\n",
    "    return books_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149570/149570 [01:55<00:00, 1298.32it/s]\n"
     ]
    }
   ],
   "source": [
    "books_ = books.copy()\n",
    "books_['img_path'] = books_['img_path'].apply(lambda x: f'data/{x}')\n",
    "img_vecs = []\n",
    "\n",
    "for path in tqdm(glob.glob('/data/ephemeral/home/jay/data/images/*.jpg')):\n",
    "    img = Image.open(path)\n",
    "    img_size = 28\n",
    "    transform = v2.Compose([\n",
    "            v2.Lambda(lambda x: x.convert('RGB') if x.mode != 'RGB' else x),\n",
    "            v2.Resize((img_size, img_size)),\n",
    "            v2.ToImage(),\n",
    "            v2.ToDtype(torch.float32, scale=True),\n",
    "            v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "        ])\n",
    "    img_vec = transform(img).numpy()\n",
    "    img_vecs.append(img_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_['img_vector'] = img_vecs\n",
    "\n",
    "user_features = []\n",
    "book_features = []\n",
    "sparse_cols = ['user_id', 'isbn'] + list(set(user_features + book_features) - {'user_id', 'isbn'})\n",
    "\n",
    "train_df = train.merge(books_, on='isbn', how='left')\\\n",
    "                .merge(users, on='user_id', how='left')[sparse_cols + ['img_vector', 'rating']]\n",
    "test_df = test.merge(books_, on='isbn', how='left')\\\n",
    "                .merge(users, on='user_id', how='left')[sparse_cols + ['img_vector']]\n",
    "all_df = pd.concat([train_df, test_df], axis=0)\n",
    "\n",
    "# feature_cols의 데이터만 라벨 인코딩하고 인덱스 정보를 저장\n",
    "label2idx, idx2label = {}, {}\n",
    "for col in sparse_cols:\n",
    "    all_df[col] = all_df[col].fillna('unknown')\n",
    "    unique_labels = all_df[col].astype(\"category\").cat.categories\n",
    "    label2idx[col] = {label:idx for idx, label in enumerate(unique_labels)}\n",
    "    idx2label[col] = {idx:label for idx, label in enumerate(unique_labels)}\n",
    "    train_df[col] = train_df[col].astype(\"category\").cat.codes\n",
    "    test_df[col] = test_df[col].astype(\"category\").cat.codes\n",
    "\n",
    "field_dims = [len(label2idx[col]) for col in sparse_cols]\n",
    "\n",
    "img_data = {\n",
    "        'train':train_df,\n",
    "        'test':test_df,\n",
    "        'field_names':sparse_cols,\n",
    "        'field_dims':field_dims,\n",
    "        'label2idx':label2idx,\n",
    "        'idx2label':idx2label,\n",
    "        'sub':sub,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                                                        img_data['train'].drop(['rating'], axis=1),\n",
    "                                                        img_data['train']['rating'],\n",
    "                                                        test_size=0.2,\n",
    "                                                        random_state=42,\n",
    "                                                        shuffle=True\n",
    "                                                        )\n",
    "img_data['X_train'], img_data['X_valid'], img_data['y_train'], img_data['y_valid'] = X_train, X_valid, y_train, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Image_Dataset(\n",
    "                            img_data['X_train'][img_data['field_names']].values,\n",
    "                            img_data['X_train']['img_vector'].values,\n",
    "                            img_data['y_train'].values\n",
    "                            )\n",
    "valid_dataset = Image_Dataset(\n",
    "                            img_data['X_valid'][img_data['field_names']].values,\n",
    "                            img_data['X_valid']['img_vector'].values,\n",
    "                            img_data['y_valid'].values\n",
    "                            )\n",
    "test_dataset = Image_Dataset(\n",
    "                            img_data['test'][img_data['field_names']].values,\n",
    "                            img_data['test']['img_vector'].values\n",
    "                            )\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)\n",
    "valid_dataloader = DataLoader(valid_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=512, shuffle=False, num_workers=0)\n",
    "img_data['train_dataloader'], img_data['valid_dataloader'], img_data['test_dataloader'] = train_dataloader, valid_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Image_Dataset at 0x7f5de0ce7bb0>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>age_range</th>\n",
       "      <th>language</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publication_range</th>\n",
       "      <th>category</th>\n",
       "      <th>book_title</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "      <th>book_author</th>\n",
       "      <th>rating</th>\n",
       "      <th>img_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4143</td>\n",
       "      <td>11</td>\n",
       "      <td>117</td>\n",
       "      <td>17674</td>\n",
       "      <td>9906</td>\n",
       "      <td>918</td>\n",
       "      <td>28</td>\n",
       "      <td>43656</td>\n",
       "      <td>4</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>14622</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4143</td>\n",
       "      <td>11</td>\n",
       "      <td>117</td>\n",
       "      <td>17674</td>\n",
       "      <td>9956</td>\n",
       "      <td>918</td>\n",
       "      <td>28</td>\n",
       "      <td>43656</td>\n",
       "      <td>7</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>26586</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4143</td>\n",
       "      <td>11</td>\n",
       "      <td>117</td>\n",
       "      <td>17674</td>\n",
       "      <td>4948</td>\n",
       "      <td>918</td>\n",
       "      <td>28</td>\n",
       "      <td>43656</td>\n",
       "      <td>8</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>42805</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4143</td>\n",
       "      <td>11</td>\n",
       "      <td>117</td>\n",
       "      <td>17674</td>\n",
       "      <td>2120</td>\n",
       "      <td>918</td>\n",
       "      <td>28</td>\n",
       "      <td>43656</td>\n",
       "      <td>8</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>45171</td>\n",
       "      <td>31</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4143</td>\n",
       "      <td>11</td>\n",
       "      <td>117</td>\n",
       "      <td>17674</td>\n",
       "      <td>3911</td>\n",
       "      <td>918</td>\n",
       "      <td>28</td>\n",
       "      <td>43656</td>\n",
       "      <td>9</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306790</th>\n",
       "      <td>59796</td>\n",
       "      <td>77468</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8494</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>96960</td>\n",
       "      <td>7717</td>\n",
       "      <td>205</td>\n",
       "      <td>198</td>\n",
       "      <td>10933</td>\n",
       "      <td>7</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306791</th>\n",
       "      <td>59800</td>\n",
       "      <td>66171</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7258</td>\n",
       "      <td>9</td>\n",
       "      <td>2477</td>\n",
       "      <td>85468</td>\n",
       "      <td>2380</td>\n",
       "      <td>1222</td>\n",
       "      <td>198</td>\n",
       "      <td>39786</td>\n",
       "      <td>6</td>\n",
       "      <td>[[[-1.6555357, -1.6555357, -1.6555357, -1.6212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306792</th>\n",
       "      <td>59800</td>\n",
       "      <td>96183</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5504</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>114704</td>\n",
       "      <td>2380</td>\n",
       "      <td>1222</td>\n",
       "      <td>198</td>\n",
       "      <td>9005</td>\n",
       "      <td>7</td>\n",
       "      <td>[[[-1.980906, -1.9637812, -1.8096584, -1.96378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306793</th>\n",
       "      <td>59800</td>\n",
       "      <td>98957</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5119</td>\n",
       "      <td>10</td>\n",
       "      <td>2086</td>\n",
       "      <td>85105</td>\n",
       "      <td>2380</td>\n",
       "      <td>1222</td>\n",
       "      <td>198</td>\n",
       "      <td>24508</td>\n",
       "      <td>7</td>\n",
       "      <td>[[[-2.117904, -2.117904, -2.1007793, -2.066529...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306794</th>\n",
       "      <td>59800</td>\n",
       "      <td>108498</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>429</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>21121</td>\n",
       "      <td>2380</td>\n",
       "      <td>1222</td>\n",
       "      <td>198</td>\n",
       "      <td>33788</td>\n",
       "      <td>10</td>\n",
       "      <td>[[[-0.88492167, -0.5253019, -0.14855716, -0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>306795 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    isbn  age_range  language  publisher  publication_range  \\\n",
       "0             0      31          2         4       4143                 11   \n",
       "1         14622      31          3         4       4143                 11   \n",
       "2         26586      31          2         4       4143                 11   \n",
       "3         42805      31          2         4       4143                 11   \n",
       "4         45171      31          2         4       4143                 11   \n",
       "...         ...     ...        ...       ...        ...                ...   \n",
       "306790    59796   77468          2         4       8494                 11   \n",
       "306791    59800   66171          3         4       7258                  9   \n",
       "306792    59800   96183          3         4       5504                  9   \n",
       "306793    59800   98957          3         4       5119                 10   \n",
       "306794    59800  108498          3         4        429                 10   \n",
       "\n",
       "        category  book_title  location_city  location_state  location_country  \\\n",
       "0            117       17674           9906             918                28   \n",
       "1            117       17674           9956             918                28   \n",
       "2            117       17674           4948             918                28   \n",
       "3            117       17674           2120             918                28   \n",
       "4            117       17674           3911             918                28   \n",
       "...          ...         ...            ...             ...               ...   \n",
       "306790        -1       96960           7717             205               198   \n",
       "306791      2477       85468           2380            1222               198   \n",
       "306792        -1      114704           2380            1222               198   \n",
       "306793      2086       85105           2380            1222               198   \n",
       "306794        -1       21121           2380            1222               198   \n",
       "\n",
       "        book_author  rating                                         img_vector  \n",
       "0             43656       4  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "1             43656       7  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "2             43656       8  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "3             43656       8  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "4             43656       9  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "...             ...     ...                                                ...  \n",
       "306790        10933       7  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "306791        39786       6  [[[-1.6555357, -1.6555357, -1.6555357, -1.6212...  \n",
       "306792         9005       7  [[[-1.980906, -1.9637812, -1.8096584, -1.96378...  \n",
       "306793        24508       7  [[[-2.117904, -2.117904, -2.1007793, -2.066529...  \n",
       "306794        33788      10  [[[-0.88492167, -0.5253019, -0.14855716, -0.01...  \n",
       "\n",
       "[306795 rows x 14 columns]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = basic_data['train'].merge(img_data['train'], on=['user_id', 'isbn', 'rating'], how = 'left')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "                                                    df.drop(['rating'], axis=1),\n",
    "                                                    df['rating'],\n",
    "                                                    test_size=0.2,\n",
    "                                                    random_state=42,\n",
    "                                                    shuffle=True\n",
    "                                                    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>age_range</th>\n",
       "      <th>language</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publication_range</th>\n",
       "      <th>category</th>\n",
       "      <th>book_title</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "      <th>book_author</th>\n",
       "      <th>img_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121312</th>\n",
       "      <td>19028</td>\n",
       "      <td>69821</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9749</td>\n",
       "      <td>9</td>\n",
       "      <td>2086</td>\n",
       "      <td>38076</td>\n",
       "      <td>382</td>\n",
       "      <td>1316</td>\n",
       "      <td>198</td>\n",
       "      <td>25807</td>\n",
       "      <td>[[[-0.95342064, -1.0219197, -0.88492167, -0.88...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265089</th>\n",
       "      <td>24291</td>\n",
       "      <td>116763</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>9326</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>79158</td>\n",
       "      <td>7879</td>\n",
       "      <td>922</td>\n",
       "      <td>198</td>\n",
       "      <td>27606</td>\n",
       "      <td>[[[0.17681314, 0.8446785, 0.17681314, -1.07329...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60236</th>\n",
       "      <td>32790</td>\n",
       "      <td>769</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4913</td>\n",
       "      <td>10</td>\n",
       "      <td>2477</td>\n",
       "      <td>68874</td>\n",
       "      <td>3304</td>\n",
       "      <td>293</td>\n",
       "      <td>198</td>\n",
       "      <td>22622</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111218</th>\n",
       "      <td>5971</td>\n",
       "      <td>38907</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>1103</td>\n",
       "      <td>11</td>\n",
       "      <td>2086</td>\n",
       "      <td>31116</td>\n",
       "      <td>534</td>\n",
       "      <td>744</td>\n",
       "      <td>198</td>\n",
       "      <td>26563</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>306001</th>\n",
       "      <td>58480</td>\n",
       "      <td>105361</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3815</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>20521</td>\n",
       "      <td>3080</td>\n",
       "      <td>-1</td>\n",
       "      <td>142</td>\n",
       "      <td>11881</td>\n",
       "      <td>[[[1.5810431, 1.5467936, 1.4611698, 1.5467936,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>119879</th>\n",
       "      <td>56208</td>\n",
       "      <td>22444</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>887</td>\n",
       "      <td>10</td>\n",
       "      <td>2086</td>\n",
       "      <td>91881</td>\n",
       "      <td>3366</td>\n",
       "      <td>62</td>\n",
       "      <td>198</td>\n",
       "      <td>25799</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259178</th>\n",
       "      <td>21634</td>\n",
       "      <td>114540</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2649</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>96259</td>\n",
       "      <td>6773</td>\n",
       "      <td>561</td>\n",
       "      <td>198</td>\n",
       "      <td>51633</td>\n",
       "      <td>[[[2.0605361, 2.1119103, 2.1975343, 2.1290352,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131932</th>\n",
       "      <td>40197</td>\n",
       "      <td>22324</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>887</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>11674</td>\n",
       "      <td>10653</td>\n",
       "      <td>744</td>\n",
       "      <td>198</td>\n",
       "      <td>21987</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146867</th>\n",
       "      <td>32276</td>\n",
       "      <td>117216</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>3579</td>\n",
       "      <td>8</td>\n",
       "      <td>2086</td>\n",
       "      <td>49925</td>\n",
       "      <td>6936</td>\n",
       "      <td>1113</td>\n",
       "      <td>64</td>\n",
       "      <td>37585</td>\n",
       "      <td>[[[-1.278791, -1.3644148, -1.3301654, -1.17604...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121958</th>\n",
       "      <td>33534</td>\n",
       "      <td>112028</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>9244</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>54460</td>\n",
       "      <td>10434</td>\n",
       "      <td>724</td>\n",
       "      <td>198</td>\n",
       "      <td>29424</td>\n",
       "      <td>[[[0.86180323, 0.70768046, 0.5364329, 0.639181...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>245436 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id    isbn  age_range  language  publisher  publication_range  \\\n",
       "121312    19028   69821          2         4       9749                  9   \n",
       "265089    24291  116763          6         4       9326                 11   \n",
       "60236     32790     769          3         4       4913                 10   \n",
       "111218     5971   38907          2         4       1103                 11   \n",
       "306001    58480  105361          4         4       3815                 10   \n",
       "...         ...     ...        ...       ...        ...                ...   \n",
       "119879    56208   22444          3         4        887                 10   \n",
       "259178    21634  114540          3         4       2649                 10   \n",
       "131932    40197   22324          5         4        887                 10   \n",
       "146867    32276  117216          2         8       3579                  8   \n",
       "121958    33534  112028          1         4       9244                 11   \n",
       "\n",
       "        category  book_title  location_city  location_state  location_country  \\\n",
       "121312      2086       38076            382            1316               198   \n",
       "265089        -1       79158           7879             922               198   \n",
       "60236       2477       68874           3304             293               198   \n",
       "111218      2086       31116            534             744               198   \n",
       "306001        -1       20521           3080              -1               142   \n",
       "...          ...         ...            ...             ...               ...   \n",
       "119879      2086       91881           3366              62               198   \n",
       "259178        -1       96259           6773             561               198   \n",
       "131932        -1       11674          10653             744               198   \n",
       "146867      2086       49925           6936            1113                64   \n",
       "121958        -1       54460          10434             724               198   \n",
       "\n",
       "        book_author                                         img_vector  \n",
       "121312        25807  [[[-0.95342064, -1.0219197, -0.88492167, -0.88...  \n",
       "265089        27606  [[[0.17681314, 0.8446785, 0.17681314, -1.07329...  \n",
       "60236         22622  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "111218        26563  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "306001        11881  [[[1.5810431, 1.5467936, 1.4611698, 1.5467936,...  \n",
       "...             ...                                                ...  \n",
       "119879        25799  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "259178        51633  [[[2.0605361, 2.1119103, 2.1975343, 2.1290352,...  \n",
       "131932        21987  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "146867        37585  [[[-1.278791, -1.3644148, -1.3301654, -1.17604...  \n",
       "121958        29424  [[[0.86180323, 0.70768046, 0.5364329, 0.639181...  \n",
       "\n",
       "[245436 rows x 13 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Image_Dataset(Dataset):\n",
    "    def __init__(self, user_book_vector, img_vector, rating=None):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        user_book_vector : np.ndarray\n",
    "            모델 학습에 사용할 유저 및 책 정보(범주형 데이터)를 입력합니다.\n",
    "        img_vector : np.ndarray\n",
    "            벡터화된 이미지 데이터를 입력합니다.\n",
    "        rating : np.ndarray\n",
    "            정답 데이터를 입력합니다.\n",
    "        \"\"\"\n",
    "        self.user_book_vector = user_book_vector\n",
    "        self.img_vector = img_vector\n",
    "        self.rating = rating\n",
    "    def __len__(self):\n",
    "        return self.user_book_vector.shape[0]\n",
    "    def __getitem__(self, i):\n",
    "        return {\n",
    "                'user_book_vector' : torch.tensor(self.user_book_vector[i], dtype=torch.long),\n",
    "                'img_vector' : torch.tensor(self.img_vector[i], dtype=torch.float32),\n",
    "                'rating' : torch.tensor(self.rating[i], dtype=torch.float32)\n",
    "                } if self.rating is not None else \\\n",
    "                {\n",
    "                'user_book_vector' : torch.tensor(self.user_book_vector[i], dtype=torch.long),\n",
    "                'img_vector' : torch.tensor(self.img_vector[i], dtype=torch.float32)\n",
    "                }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = Image_Dataset(\n",
    "            X_train[['user_id', 'isbn']].values,\n",
    "            X_train['img_vector'].values,\n",
    "            y_train.values\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f5db7f2d090>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DataLoader(train_dataset, batch_size=512, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>img_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31332</th>\n",
       "      <td>2681</td>\n",
       "      <td>29051</td>\n",
       "      <td>[[[-1.1589177, -0.9362959, -0.7136741, -0.6794...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114196</th>\n",
       "      <td>18124</td>\n",
       "      <td>47681</td>\n",
       "      <td>[[[0.70768046, -0.37117895, -0.47392747, -0.40...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302635</th>\n",
       "      <td>53700</td>\n",
       "      <td>69367</td>\n",
       "      <td>[[[-1.9124069, -1.9466565, -1.6726604, -1.6555...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279523</th>\n",
       "      <td>37348</td>\n",
       "      <td>41454</td>\n",
       "      <td>[[[2.1632848, 2.1804094, 2.1632848, 2.1632848,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19094</th>\n",
       "      <td>22876</td>\n",
       "      <td>9230</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89162</th>\n",
       "      <td>11143</td>\n",
       "      <td>44370</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8960</th>\n",
       "      <td>40598</td>\n",
       "      <td>9079</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>11351</td>\n",
       "      <td>19183</td>\n",
       "      <td>[[[-1.4157891, -1.4157891, -1.3815396, -0.0115...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93315</th>\n",
       "      <td>39902</td>\n",
       "      <td>17666</td>\n",
       "      <td>[[[-0.16568191, -0.045808643, -0.21705617, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>31040</td>\n",
       "      <td>68218</td>\n",
       "      <td>[[[0.8960528, 1.0501755, 0.1939379, -0.8506721...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61359 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id   isbn                                         img_vector\n",
       "31332      2681  29051  [[[-1.1589177, -0.9362959, -0.7136741, -0.6794...\n",
       "114196    18124  47681  [[[0.70768046, -0.37117895, -0.47392747, -0.40...\n",
       "302635    53700  69367  [[[-1.9124069, -1.9466565, -1.6726604, -1.6555...\n",
       "279523    37348  41454  [[[2.1632848, 2.1804094, 2.1632848, 2.1632848,...\n",
       "19094     22876   9230  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...\n",
       "...         ...    ...                                                ...\n",
       "89162     11143  44370  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...\n",
       "8960      40598   9079  [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...\n",
       "2188      11351  19183  [[[-1.4157891, -1.4157891, -1.3815396, -0.0115...\n",
       "93315     39902  17666  [[[-0.16568191, -0.045808643, -0.21705617, -0....\n",
       "2856      31040  68218  [[[0.8960528, 1.0501755, 0.1939379, -0.8506721...\n",
       "\n",
       "[61359 rows x 3 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data['X_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>isbn</th>\n",
       "      <th>age_range</th>\n",
       "      <th>language</th>\n",
       "      <th>publisher</th>\n",
       "      <th>publication_range</th>\n",
       "      <th>category</th>\n",
       "      <th>book_title</th>\n",
       "      <th>location_city</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_country</th>\n",
       "      <th>book_author</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>31332</th>\n",
       "      <td>2681</td>\n",
       "      <td>29051</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9749</td>\n",
       "      <td>10</td>\n",
       "      <td>186</td>\n",
       "      <td>2223</td>\n",
       "      <td>2116</td>\n",
       "      <td>912</td>\n",
       "      <td>198</td>\n",
       "      <td>15453</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114196</th>\n",
       "      <td>18124</td>\n",
       "      <td>47681</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>8459</td>\n",
       "      <td>9</td>\n",
       "      <td>-1</td>\n",
       "      <td>18946</td>\n",
       "      <td>318</td>\n",
       "      <td>723</td>\n",
       "      <td>198</td>\n",
       "      <td>22117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302635</th>\n",
       "      <td>53700</td>\n",
       "      <td>69367</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>7709</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>31120</td>\n",
       "      <td>2116</td>\n",
       "      <td>912</td>\n",
       "      <td>198</td>\n",
       "      <td>43971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279523</th>\n",
       "      <td>37348</td>\n",
       "      <td>41454</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2434</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>22323</td>\n",
       "      <td>3998</td>\n",
       "      <td>912</td>\n",
       "      <td>198</td>\n",
       "      <td>26976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19094</th>\n",
       "      <td>22876</td>\n",
       "      <td>9230</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>7025</td>\n",
       "      <td>11</td>\n",
       "      <td>-1</td>\n",
       "      <td>36672</td>\n",
       "      <td>1874</td>\n",
       "      <td>558</td>\n",
       "      <td>198</td>\n",
       "      <td>49837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89162</th>\n",
       "      <td>11143</td>\n",
       "      <td>44370</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9886</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>35926</td>\n",
       "      <td>3845</td>\n",
       "      <td>183</td>\n",
       "      <td>28</td>\n",
       "      <td>28882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8960</th>\n",
       "      <td>40598</td>\n",
       "      <td>9079</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>7025</td>\n",
       "      <td>10</td>\n",
       "      <td>2086</td>\n",
       "      <td>13682</td>\n",
       "      <td>9415</td>\n",
       "      <td>301</td>\n",
       "      <td>198</td>\n",
       "      <td>19979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2188</th>\n",
       "      <td>11351</td>\n",
       "      <td>19183</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5453</td>\n",
       "      <td>10</td>\n",
       "      <td>2086</td>\n",
       "      <td>87576</td>\n",
       "      <td>8092</td>\n",
       "      <td>744</td>\n",
       "      <td>198</td>\n",
       "      <td>21723</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93315</th>\n",
       "      <td>39902</td>\n",
       "      <td>17666</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>8693</td>\n",
       "      <td>10</td>\n",
       "      <td>-1</td>\n",
       "      <td>69303</td>\n",
       "      <td>1170</td>\n",
       "      <td>409</td>\n",
       "      <td>198</td>\n",
       "      <td>8725</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2856</th>\n",
       "      <td>31040</td>\n",
       "      <td>68218</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8735</td>\n",
       "      <td>10</td>\n",
       "      <td>2086</td>\n",
       "      <td>72100</td>\n",
       "      <td>10238</td>\n",
       "      <td>1341</td>\n",
       "      <td>198</td>\n",
       "      <td>36821</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>61359 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        user_id   isbn  age_range  language  publisher  publication_range  \\\n",
       "31332      2681  29051          2         4       9749                 10   \n",
       "114196    18124  47681          1         4       8459                  9   \n",
       "302635    53700  69367          5         4       7709                 10   \n",
       "279523    37348  41454          3         4       2434                 10   \n",
       "19094     22876   9230          3         4       7025                 11   \n",
       "...         ...    ...        ...       ...        ...                ...   \n",
       "89162     11143  44370          2         4       9886                 10   \n",
       "8960      40598   9079          4         4       7025                 10   \n",
       "2188      11351  19183          3         4       5453                 10   \n",
       "93315     39902  17666          2         4       8693                 10   \n",
       "2856      31040  68218          3         4       8735                 10   \n",
       "\n",
       "        category  book_title  location_city  location_state  location_country  \\\n",
       "31332        186        2223           2116             912               198   \n",
       "114196        -1       18946            318             723               198   \n",
       "302635        -1       31120           2116             912               198   \n",
       "279523        -1       22323           3998             912               198   \n",
       "19094         -1       36672           1874             558               198   \n",
       "...          ...         ...            ...             ...               ...   \n",
       "89162         -1       35926           3845             183                28   \n",
       "8960        2086       13682           9415             301               198   \n",
       "2188        2086       87576           8092             744               198   \n",
       "93315         -1       69303           1170             409               198   \n",
       "2856        2086       72100          10238            1341               198   \n",
       "\n",
       "        book_author  \n",
       "31332         15453  \n",
       "114196        22117  \n",
       "302635        43971  \n",
       "279523        26976  \n",
       "19094         49837  \n",
       "...             ...  \n",
       "89162         28882  \n",
       "8960          19979  \n",
       "2188          21723  \n",
       "93315          8725  \n",
       "2856          36821  \n",
       "\n",
       "[61359 rows x 12 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "basic_data['X_valid']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['train', 'test', 'field_names', 'field_dims', 'label2idx', 'idx2label', 'sub', 'X_train', 'X_valid', 'y_train', 'y_valid', 'train_dataloader', 'valid_dataloader', 'test_dataloader'])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_train_dataset = TensorDataset(torch.LongTensor(basic_data['X_train'].values), torch.LongTensor(basic_data['y_train'].values))\n",
    "img_train_dataset = Image_Dataset(\n",
    "            X_train[['user_id', 'isbn']].values,\n",
    "            X_train['img_vector'].values,\n",
    "            y_train.values\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "combined_dataset = ConcatDataset([basic_train_dataset, img_train_dataset])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(combined_dataset, batch_size=512, shuffle=True, num_workers=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2352"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(books_['img_vector'][0].flatten() == books_['img_vector'][2].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>isbn</th>\n",
       "      <th>book_title</th>\n",
       "      <th>book_author</th>\n",
       "      <th>year_of_publication</th>\n",
       "      <th>publisher</th>\n",
       "      <th>img_url</th>\n",
       "      <th>language</th>\n",
       "      <th>category</th>\n",
       "      <th>summary</th>\n",
       "      <th>img_path</th>\n",
       "      <th>img_vector</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0002005018</td>\n",
       "      <td>Clara Callan</td>\n",
       "      <td>Richard Bruce Wright</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>HarperFlamingo Canada</td>\n",
       "      <td>http://images.amazon.com/images/P/0002005018.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Actresses']</td>\n",
       "      <td>In a small town in Canada, Clara Callan reluct...</td>\n",
       "      <td>data/images/0002005018.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0060973129</td>\n",
       "      <td>Decision in Normandy</td>\n",
       "      <td>Carlo D'Este</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>HarperPerennial</td>\n",
       "      <td>http://images.amazon.com/images/P/0060973129.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['1940-1949']</td>\n",
       "      <td>Here, for the first time in paperback, is an o...</td>\n",
       "      <td>data/images/0060973129.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[-1.6384109, -1.6555357, -1.7069099, -1.6897...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0374157065</td>\n",
       "      <td>Flu: The Story of the Great Influenza Pandemic...</td>\n",
       "      <td>Gina Bari Kolata</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>Farrar Straus Giroux</td>\n",
       "      <td>http://images.amazon.com/images/P/0374157065.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Medical']</td>\n",
       "      <td>Describes the great flu epidemic of 1918, an o...</td>\n",
       "      <td>data/images/0374157065.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0399135782</td>\n",
       "      <td>The Kitchen God's Wife</td>\n",
       "      <td>Amy Tan</td>\n",
       "      <td>1991.0</td>\n",
       "      <td>Putnam Pub Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0399135782.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>A Chinese immigrant who is convinced she is dy...</td>\n",
       "      <td>data/images/0399135782.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0425176428</td>\n",
       "      <td>What If?: The World's Foremost Military Histor...</td>\n",
       "      <td>Robert Cowley</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>Berkley Publishing Group</td>\n",
       "      <td>http://images.amazon.com/images/P/0425176428.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['History']</td>\n",
       "      <td>Essays by respected military historians, inclu...</td>\n",
       "      <td>data/images/0425176428.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149565</th>\n",
       "      <td>067161746X</td>\n",
       "      <td>The Bachelor Home Companion: A Practical Guide...</td>\n",
       "      <td>P.J. O'Rourke</td>\n",
       "      <td>1987.0</td>\n",
       "      <td>Pocket Books</td>\n",
       "      <td>http://images.amazon.com/images/P/067161746X.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Humor']</td>\n",
       "      <td>A tongue-in-cheek survival guide for single pe...</td>\n",
       "      <td>data/images/067161746X.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[-1.6555357, -1.6555357, -1.6555357, -1.6212...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149566</th>\n",
       "      <td>0767907566</td>\n",
       "      <td>All Elevations Unknown: An Adventure in the He...</td>\n",
       "      <td>Sam Lightner</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>Broadway Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0767907566.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Nature']</td>\n",
       "      <td>A daring twist on the travel-adventure genre t...</td>\n",
       "      <td>data/images/0767907566.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[1.2727976, 1.3926709, 1.0330508, 0.9816765,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149567</th>\n",
       "      <td>0884159221</td>\n",
       "      <td>Why stop?: A guide to Texas historical roadsid...</td>\n",
       "      <td>Claude Dooley</td>\n",
       "      <td>1985.0</td>\n",
       "      <td>Lone Star Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0884159221.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/images/0884159221.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[-1.980906, -1.9637812, -1.8096584, -1.96378...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149568</th>\n",
       "      <td>0912333022</td>\n",
       "      <td>The Are You Being Served? Stories: 'Camping In...</td>\n",
       "      <td>Jeremy Lloyd</td>\n",
       "      <td>1997.0</td>\n",
       "      <td>Kqed Books</td>\n",
       "      <td>http://images.amazon.com/images/P/0912333022.0...</td>\n",
       "      <td>en</td>\n",
       "      <td>['Fiction']</td>\n",
       "      <td>These hilarious stories by the creator of publ...</td>\n",
       "      <td>data/images/0912333022.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[-2.117904, -2.117904, -2.1007793, -2.066529...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149569</th>\n",
       "      <td>1569661057</td>\n",
       "      <td>Dallas Street Map Guide and Directory, 2000 Ed...</td>\n",
       "      <td>Mapsco</td>\n",
       "      <td>1999.0</td>\n",
       "      <td>American Map Corporation</td>\n",
       "      <td>http://images.amazon.com/images/P/1569661057.0...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>data/images/1569661057.01.THUMBZZZ.jpg</td>\n",
       "      <td>[[[-0.88492167, -0.5253019, -0.14855716, -0.01...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>149570 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              isbn                                         book_title  \\\n",
       "0       0002005018                                       Clara Callan   \n",
       "1       0060973129                               Decision in Normandy   \n",
       "2       0374157065  Flu: The Story of the Great Influenza Pandemic...   \n",
       "3       0399135782                             The Kitchen God's Wife   \n",
       "4       0425176428  What If?: The World's Foremost Military Histor...   \n",
       "...            ...                                                ...   \n",
       "149565  067161746X  The Bachelor Home Companion: A Practical Guide...   \n",
       "149566  0767907566  All Elevations Unknown: An Adventure in the He...   \n",
       "149567  0884159221  Why stop?: A guide to Texas historical roadsid...   \n",
       "149568  0912333022  The Are You Being Served? Stories: 'Camping In...   \n",
       "149569  1569661057  Dallas Street Map Guide and Directory, 2000 Ed...   \n",
       "\n",
       "                 book_author  year_of_publication                 publisher  \\\n",
       "0       Richard Bruce Wright               2001.0     HarperFlamingo Canada   \n",
       "1               Carlo D'Este               1991.0           HarperPerennial   \n",
       "2           Gina Bari Kolata               1999.0      Farrar Straus Giroux   \n",
       "3                    Amy Tan               1991.0          Putnam Pub Group   \n",
       "4              Robert Cowley               2000.0  Berkley Publishing Group   \n",
       "...                      ...                  ...                       ...   \n",
       "149565         P.J. O'Rourke               1987.0              Pocket Books   \n",
       "149566          Sam Lightner               2001.0            Broadway Books   \n",
       "149567         Claude Dooley               1985.0           Lone Star Books   \n",
       "149568          Jeremy Lloyd               1997.0                Kqed Books   \n",
       "149569                Mapsco               1999.0  American Map Corporation   \n",
       "\n",
       "                                                  img_url language  \\\n",
       "0       http://images.amazon.com/images/P/0002005018.0...       en   \n",
       "1       http://images.amazon.com/images/P/0060973129.0...       en   \n",
       "2       http://images.amazon.com/images/P/0374157065.0...       en   \n",
       "3       http://images.amazon.com/images/P/0399135782.0...       en   \n",
       "4       http://images.amazon.com/images/P/0425176428.0...       en   \n",
       "...                                                   ...      ...   \n",
       "149565  http://images.amazon.com/images/P/067161746X.0...       en   \n",
       "149566  http://images.amazon.com/images/P/0767907566.0...       en   \n",
       "149567  http://images.amazon.com/images/P/0884159221.0...      NaN   \n",
       "149568  http://images.amazon.com/images/P/0912333022.0...       en   \n",
       "149569  http://images.amazon.com/images/P/1569661057.0...      NaN   \n",
       "\n",
       "             category                                            summary  \\\n",
       "0       ['Actresses']  In a small town in Canada, Clara Callan reluct...   \n",
       "1       ['1940-1949']  Here, for the first time in paperback, is an o...   \n",
       "2         ['Medical']  Describes the great flu epidemic of 1918, an o...   \n",
       "3         ['Fiction']  A Chinese immigrant who is convinced she is dy...   \n",
       "4         ['History']  Essays by respected military historians, inclu...   \n",
       "...               ...                                                ...   \n",
       "149565      ['Humor']  A tongue-in-cheek survival guide for single pe...   \n",
       "149566     ['Nature']  A daring twist on the travel-adventure genre t...   \n",
       "149567            NaN                                                NaN   \n",
       "149568    ['Fiction']  These hilarious stories by the creator of publ...   \n",
       "149569            NaN                                                NaN   \n",
       "\n",
       "                                      img_path  \\\n",
       "0       data/images/0002005018.01.THUMBZZZ.jpg   \n",
       "1       data/images/0060973129.01.THUMBZZZ.jpg   \n",
       "2       data/images/0374157065.01.THUMBZZZ.jpg   \n",
       "3       data/images/0399135782.01.THUMBZZZ.jpg   \n",
       "4       data/images/0425176428.01.THUMBZZZ.jpg   \n",
       "...                                        ...   \n",
       "149565  data/images/067161746X.01.THUMBZZZ.jpg   \n",
       "149566  data/images/0767907566.01.THUMBZZZ.jpg   \n",
       "149567  data/images/0884159221.01.THUMBZZZ.jpg   \n",
       "149568  data/images/0912333022.01.THUMBZZZ.jpg   \n",
       "149569  data/images/1569661057.01.THUMBZZZ.jpg   \n",
       "\n",
       "                                               img_vector  \n",
       "0       [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "1       [[[-1.6384109, -1.6555357, -1.7069099, -1.6897...  \n",
       "2       [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "3       [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "4       [[[2.2489083, 2.2489083, 2.2489083, 2.2489083,...  \n",
       "...                                                   ...  \n",
       "149565  [[[-1.6555357, -1.6555357, -1.6555357, -1.6212...  \n",
       "149566  [[[1.2727976, 1.3926709, 1.0330508, 0.9816765,...  \n",
       "149567  [[[-1.980906, -1.9637812, -1.8096584, -1.96378...  \n",
       "149568  [[[-2.117904, -2.117904, -2.1007793, -2.066529...  \n",
       "149569  [[[-0.88492167, -0.5253019, -0.14855716, -0.01...  \n",
       "\n",
       "[149570 rows x 11 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        self.layer1 = nn.Sequential(#in_channels = 3이면 R,G,B 흑백이면 1\n",
    "            nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "        self.fc1 = nn.Linear(64*6*6,600)\n",
    "        self.dropout = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(600,120)\n",
    "        self.fc3 = nn.Linear(120,10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc1(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 2.0829, Accuracy: 23.88%\n",
      "Epoch [2/10], Loss: 2.0757, Accuracy: 24.06%\n",
      "Epoch [3/10], Loss: 2.0755, Accuracy: 24.06%\n",
      "Epoch [4/10], Loss: 2.0751, Accuracy: 24.08%\n",
      "Epoch [5/10], Loss: 2.0741, Accuracy: 24.09%\n",
      "Epoch [6/10], Loss: 2.0733, Accuracy: 24.10%\n",
      "Epoch [7/10], Loss: 2.0734, Accuracy: 24.09%\n",
      "Epoch [8/10], Loss: 2.0719, Accuracy: 24.12%\n",
      "Epoch [9/10], Loss: 2.0706, Accuracy: 24.13%\n",
      "Epoch [10/10], Loss: 2.0693, Accuracy: 24.15%\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# 모델을 GPU 또는 CPU로 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# img_vector 열을 torch.Tensor로 변환한 후, torch.stack 사용\n",
    "X_train = torch.stack([torch.tensor(img) for img in df['img_vector']])  # (100, 3, 28, 28) 크기의 텐서로 변환\n",
    "y_train = torch.tensor(df['rating'].values) - 1  # rating을 텐서로 변환\n",
    "\n",
    "# 데이터셋을 TensorDataset으로 만들고, DataLoader로 로드\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "# 모델 초기화 후, GPU로 이동\n",
    "model = CNN().to(device)  # CNN 모델을 GPU로 이동\n",
    "\n",
    "# 손실 함수와 옵티마이저 초기화\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 학습 과정\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 설정\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for inputs, labels in train_loader:\n",
    "        # 데이터를 GPU로 이동\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        # 기울기 초기화\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # 모델을 통해 예측값 계산\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # 손실 계산\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # 역전파\n",
    "        loss.backward()\n",
    "\n",
    "        # 옵티마이저로 가중치 업데이트\n",
    "        optimizer.step()\n",
    "\n",
    "        # 통계 출력\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_loss = running_loss / len(train_loader)\n",
    "    epoch_accuracy = 100 * correct / total\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.2f}%\")\n",
    "\n",
    "print(\"Finished Training\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_ = users.copy()\n",
    "books_ = books.copy()\n",
    "\n",
    "# 데이터 전처리 (전처리는 각자의 상황에 맞게 진행해주세요!)\n",
    "books_['category'] = books_['category'].apply(lambda x: str2list(x)[0] if not pd.isna(x) else np.nan)\n",
    "books_['language'] = books_['language'].fillna(books_['language'].mode()[0])\n",
    "books_['publication_range'] = books_['year_of_publication'].apply(lambda x: x // 10 * 10)  # 1990년대, 2000년대, 2010년대, ...\n",
    "\n",
    "users_['age'] = users_['age'].fillna(users_['age'].mode()[0])\n",
    "users_['age_range'] = users_['age'].apply(lambda x: x // 10 * 10)  # 10대, 20대, 30대, ...\n",
    "\n",
    "users_['location_list'] = users_['location'].apply(lambda x: split_location(x)) \n",
    "users_['location_country'] = users_['location_list'].apply(lambda x: x[0])\n",
    "users_['location_state'] = users_['location_list'].apply(lambda x: x[1] if len(x) > 1 else np.nan)\n",
    "users_['location_city'] = users_['location_list'].apply(lambda x: x[2] if len(x) > 2 else np.nan)\n",
    "for idx, row in users_.iterrows():\n",
    "    if (not pd.isna(row['location_state'])) and pd.isna(row['location_country']):\n",
    "        fill_country = users_[users_['location_state'] == row['location_state']]['location_country'].mode()\n",
    "        fill_country = fill_country[0] if len(fill_country) > 0 else np.nan\n",
    "        users_.loc[idx, 'location_country'] = fill_country\n",
    "    elif (not pd.isna(row['location_city'])) and pd.isna(row['location_state']):\n",
    "        if not pd.isna(row['location_country']):\n",
    "            fill_state = users_[(users_['location_country'] == row['location_country']) \n",
    "                                & (users_['location_city'] == row['location_city'])]['location_state'].mode()\n",
    "            fill_state = fill_state[0] if len(fill_state) > 0 else np.nan\n",
    "            users_.loc[idx, 'location_state'] = fill_state\n",
    "        else:\n",
    "            fill_state = users_[users_['location_city'] == row['location_city']]['location_state'].mode()\n",
    "            fill_state = fill_state[0] if len(fill_state) > 0 else np.nan\n",
    "            fill_country = users_[users_['location_city'] == row['location_city']]['location_country'].mode()\n",
    "            fill_country = fill_country[0] if len(fill_country) > 0 else np.nan\n",
    "            users_.loc[idx, 'location_country'] = fill_country\n",
    "            users_.loc[idx, 'location_state'] = fill_state\n",
    "\n",
    "            \n",
    "\n",
    "users_ = users_.drop(['location'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>age</th>\n",
       "      <th>age_range</th>\n",
       "      <th>location_list</th>\n",
       "      <th>location_country</th>\n",
       "      <th>location_state</th>\n",
       "      <th>location_city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8</td>\n",
       "      <td>29.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[canada, ontario, timmins]</td>\n",
       "      <td>canada</td>\n",
       "      <td>ontario</td>\n",
       "      <td>timmins</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11400</td>\n",
       "      <td>49.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>[canada, ontario, ottawa]</td>\n",
       "      <td>canada</td>\n",
       "      <td>ontario</td>\n",
       "      <td>ottawa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11676</td>\n",
       "      <td>29.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[nan, nan, nan]</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>67544</td>\n",
       "      <td>30.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>[canada, ontario, toronto]</td>\n",
       "      <td>canada</td>\n",
       "      <td>ontario</td>\n",
       "      <td>toronto</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>85526</td>\n",
       "      <td>36.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>[canada, british columbia, victoria]</td>\n",
       "      <td>canada</td>\n",
       "      <td>british columbia</td>\n",
       "      <td>victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68087</th>\n",
       "      <td>278376</td>\n",
       "      <td>54.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>[usa, pennsylvania, danville]</td>\n",
       "      <td>usa</td>\n",
       "      <td>pennsylvania</td>\n",
       "      <td>danville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68088</th>\n",
       "      <td>278621</td>\n",
       "      <td>74.0</td>\n",
       "      <td>70.0</td>\n",
       "      <td>[canada, delaware, victoria]</td>\n",
       "      <td>canada</td>\n",
       "      <td>delaware</td>\n",
       "      <td>victoria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68089</th>\n",
       "      <td>278636</td>\n",
       "      <td>29.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>[usa, alabama, irvington]</td>\n",
       "      <td>usa</td>\n",
       "      <td>alabama</td>\n",
       "      <td>irvington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68090</th>\n",
       "      <td>278659</td>\n",
       "      <td>33.0</td>\n",
       "      <td>30.0</td>\n",
       "      <td>[usa, washington, vancouver]</td>\n",
       "      <td>usa</td>\n",
       "      <td>washington</td>\n",
       "      <td>vancouver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68091</th>\n",
       "      <td>278713</td>\n",
       "      <td>63.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>[usa, new mexico, albuquerque]</td>\n",
       "      <td>usa</td>\n",
       "      <td>new mexico</td>\n",
       "      <td>albuquerque</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68092 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       user_id   age  age_range                         location_list  \\\n",
       "0            8  29.0       20.0            [canada, ontario, timmins]   \n",
       "1        11400  49.0       40.0             [canada, ontario, ottawa]   \n",
       "2        11676  29.0       20.0                       [nan, nan, nan]   \n",
       "3        67544  30.0       30.0            [canada, ontario, toronto]   \n",
       "4        85526  36.0       30.0  [canada, british columbia, victoria]   \n",
       "...        ...   ...        ...                                   ...   \n",
       "68087   278376  54.0       50.0         [usa, pennsylvania, danville]   \n",
       "68088   278621  74.0       70.0          [canada, delaware, victoria]   \n",
       "68089   278636  29.0       20.0             [usa, alabama, irvington]   \n",
       "68090   278659  33.0       30.0          [usa, washington, vancouver]   \n",
       "68091   278713  63.0       60.0        [usa, new mexico, albuquerque]   \n",
       "\n",
       "      location_country    location_state location_city  \n",
       "0               canada           ontario       timmins  \n",
       "1               canada           ontario        ottawa  \n",
       "2                  NaN               NaN           NaN  \n",
       "3               canada           ontario       toronto  \n",
       "4               canada  british columbia      victoria  \n",
       "...                ...               ...           ...  \n",
       "68087              usa      pennsylvania      danville  \n",
       "68088           canada          delaware      victoria  \n",
       "68089              usa           alabama     irvington  \n",
       "68090              usa        washington     vancouver  \n",
       "68091              usa        new mexico   albuquerque  \n",
       "\n",
       "[68092 rows x 7 columns]"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "users_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
